{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNYzTx3CY0rJ"
      },
      "source": [
        "# Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_CGCz09uTvOA",
        "outputId": "c01d9e1b-561e-4310-bd15-ea0473c7d976"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-groq transformers\n",
        "!pip install llama-index\n",
        "!pip install langchain-experimental\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUFnAeeEU6qx"
      },
      "source": [
        "# Step 1: Load and preprocess text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "collapsed": true,
        "id": "JQKMiBugU50P",
        "outputId": "57a78b84-34b4-4d93-88f7-097e4fe33453"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Load text data\n",
        "text = \"\"\"Sarah is an employee at prismaticAI, a leading technology company based in Westside Valley. She has been working there for the past three years as a software engineer.\n",
        "Michael is also an employee at prismaticAI, where he works as a data scientist. He joined the company two years ago after completing his graduate studies.\n",
        "prismaticAI is a well-known technology company that specializes in developing cutting-edge software solutions and artificial intelligence applications. The company has a diverse workforce of talented individuals from various backgrounds.\n",
        "Both Sarah and Michael are highly skilled professionals who contribute significantly to prismaticAI's success. They work closely with their respective teams to develop innovative products and services that meet the evolving needs of the company's clients.\"\"\"\n",
        "\n",
        "\n",
        "documents = [Document(page_content=text)]\n",
        "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Fr_EyuXXYJ"
      },
      "source": [
        "# Step 2: Initialize language model and extract knowledge graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "mS9iexEeXW9C",
        "outputId": "590fc23a-7d53-4ad3-8f9e-0ab0e3997f5f"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# Set Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
        "\n",
        "# Initialize Groq LLM\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
        ")\n",
        "\n",
        "# Extract Knowledge Graph\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "\n",
        "# Assuming you have your texts defined\n",
        "graph_documents = llm_transformer.convert_to_graph_documents(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5ih701uX2__"
      },
      "source": [
        "# Step 3: Store knowledge graph in a database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install neo4j langchain_neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "xrtL-ZwwX2Xg"
      },
      "outputs": [],
      "source": [
        "from langchain_neo4j import Neo4jGraph\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get Neo4j connection details from environment variables\n",
        "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
        "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
        "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
        "\n",
        "# Initialize the Neo4j graph wrapper\n",
        "graph = Neo4jGraph(\n",
        "    url=NEO4J_URI,\n",
        "    username=NEO4J_USERNAME,\n",
        "    password=NEO4J_PASSWORD,\n",
        ")\n",
        "\n",
        "# Persist your extracted GraphDocument objects into Neo4j\n",
        "graph.add_graph_documents(\n",
        "    graph_documents,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bl6Dc70X58Y"
      },
      "source": [
        "# Step 4: Retrieve knowledge for RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Lsp_r6-SX61w"
      },
      "outputs": [],
      "source": [
        "from langchain_neo4j import GraphCypherQAChain\n",
        "\n",
        "\n",
        "# Create the RAG chain with Cypher-based retrieval\n",
        "chain = GraphCypherQAChain.from_llm(\n",
        "    llm=llm,\n",
        "    graph=graph,\n",
        "    verbose=True,\n",
        "    allow_dangerous_requests=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8s3VjcEX9dI"
      },
      "source": [
        "# Step 5: Query the knowledge graph and generate a response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "UfArNBfAX-aJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "MATCH (m:Person {id: \"Michael\"})-[:EMPLOYEE]->(o:Organization)\n",
            "MATCH (s:Person {id: \"Sarah\"})-[:EMPLOYEE]->(o2:Organization)\n",
            "RETURN o.id = o2.id AS result\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'result': True}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Yes, Michael works for the same company as Sarah.'"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def query_graph(query):\n",
        "    try:\n",
        "        response = chain.invoke({\"query\": query})\n",
        "        return response[\"result\"]\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "query_graph(\"Does Michael work for the same company as Sarah?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference with gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7864\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 213, which is longer than the specified 200\n",
            "Created a chunk of size 318, which is longer than the specified 200\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (o:Occupation {id: \"AI Agent\"}) RETURN o\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "MATCH (m:Person {id: \"Michael\"})-[:EMPLOYEE]->(o:Organization)\n",
            "MATCH (s:Person {id: \"Sarah\"})-[:EMPLOYEE]->(o2:Organization)\n",
            "RETURN o.id = o2.id AS result\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'result': True}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "import os\n",
        "\n",
        "qa_chain  = GraphCypherQAChain.from_llm(\n",
        "    llm=llm,\n",
        "    graph=graph,\n",
        "    verbose=True,\n",
        "    allow_dangerous_requests=True\n",
        ")\n",
        "\n",
        "# Handle file upload and process it into Neo4j\n",
        "def upload_document(file):\n",
        "    _, ext = os.path.splitext(file.name)\n",
        "    \n",
        "    if ext.lower() == \".pdf\":\n",
        "        loader = PyPDFLoader(file.name)\n",
        "    elif ext.lower() == \".txt\":\n",
        "        loader = TextLoader(file.name)\n",
        "    else:\n",
        "        return \"Unsupported file type. Please upload a PDF or TXT file.\"\n",
        "    \n",
        "    documents = loader.load()\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    graph_documents = llm_transformer.convert_to_graph_documents(texts)\n",
        "\n",
        "    graph.add_graph_documents(\n",
        "        graph_documents,\n",
        "    )\n",
        "    return \"✅ Document processed and added to the knowledge graph!\"\n",
        "\n",
        "# Ask a question\n",
        "def query_graph(query):\n",
        "    try:\n",
        "        response = qa_chain.invoke({\"query\": query})\n",
        "        return response[\"result\"]\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error: {str(e)}\"\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 📄 Upload a Document & Ask Questions\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        file_upload = gr.File(label=\"Upload PDF or TXT\", file_types=[\".pdf\", \".txt\"])\n",
        "        upload_btn = gr.Button(\"Process and Extract\")\n",
        "        upload_output = gr.Textbox(label=\"Upload Status\")\n",
        "\n",
        "    with gr.Row():\n",
        "        query_input = gr.Textbox(label=\"Ask a Question\")\n",
        "        query_output = gr.Textbox(label=\"Answer\")\n",
        "        ask_btn = gr.Button(\"Ask\")\n",
        "\n",
        "    upload_btn.click(fn=upload_document, inputs=file_upload, outputs=upload_output)\n",
        "    ask_btn.click(fn=query_graph, inputs=query_input, outputs=query_output)\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "o4Fr_EyuXXYJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
